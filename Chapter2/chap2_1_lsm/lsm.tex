\documentclass[12pt]{jsarticle}
\usepackage[dvips]{graphicx}
\usepackage{amsmath}

\title{最小二乗法}
\author{大枝 真一}
\date{}

\begin{document}
\maketitle

\section{はじめに}
複雑なデータや関数を簡単な関数の和で近似する代表的な手法が「最小二乗法」
である．これはコンピュータによるデータ解析の最も重要な基礎である．本授業
ではこれを学ぶと共に，ベクトルや行列による線形計算に慣れることを目的とす
る．

\section{最小二乗法とは}
$N$個のデータ$(x_1, y_1), \ldots, (x_N, y_N)$に直線を当てはめたいとする．
当てはめたい直線を$y=ax+b$と置く．$a, b$はこれから定める未知の定数である．

理想的には$y_\alpha=ax_\alpha+b, \alpha=1, \ldots, N$，となることが望ま
しいが，データ点${(x_\alpha, y_\alpha)}$が厳密に同一直線上にあるとは限ら
ないので，$a, b$をどう選んでも多くの$\alpha$に対して
$y_\alpha \neq ax_\alpha+b$となる．そこで

\begin{equation}
 y_\alpha \approx ax_\alpha+b, \hspace{0.7cm} \alpha=1,\ldots, N 
\end{equation}

となるように$a, b$を定める(図\ref{fig:line})．記号$\approx$は「ほぼ等し
い」という意味である．これを次のように解釈する．ただし$\rightarrow$はそ
の左側の式を最小にすることを表す．

\begin{equation}
 J(a,b)=\frac{1}{2}\sum_{\alpha=1}^N(y_\alpha - (ax_\alpha+b))^2 \rightarrow
  \min
  \label{equ:min}
\end{equation}

\begin{figure}[ht]
 \begin{center}
  \includegraphics[clip,scale=0.8]{fig/line.eps}
 \end{center}
 \caption{直線の当てはめ}
 \label{fig:line}
\end{figure}

これは食い違いの二乗の和を最小にする方法であることから，\textgt{最小二乗
法}と呼ばれている．全体を$1/2$倍するのは後の計算を見やすくするためである．

\subsection{1次の最小二乗法の正規方程式}
\begin{center}
\begin{tabular}{|c|} \hline
 $N$個のデータ$(x_1, y_1), \ldots, (x_N, y_N)$に直線$y=ax+b$を当てはめよ．
 \\ \hline
\end{tabular}
\end{center}
\vspace{5mm}
(解)

式(\ref{equ:min})は$a, b$の関数である.解析学で知られるように多変数の関
数が最大値や最小値をとる点では，各変数に関する偏導関数が0でなければなら
ない．したがって，

\begin{equation}
 \frac{\partial J(a,b)}{\partial a}=0, \hspace{0.7cm} \frac{\partial
  J(a,b)}{\partial b}=0
\end{equation}

を解いて$a, b$を定めればよい．式(\ref{equ:min})を$a, b$でそれぞれ偏微分
すると次式を得る．

\[
 \frac{\partial J(a,b)}{\partial a} = \sum_{\alpha=1}^N(y_\alpha -
  ax_\alpha-b)(-x_\alpha) = a \sum_{\alpha=1}^N x_\alpha ^2 +
  b\sum_{\alpha=1}^N x_\alpha - \sum_{\alpha=1}^N x_\alpha y_\alpha = 0
\]
\begin{equation}
 \frac{\partial J(a,b)}{\partial b} = \sum_{\alpha=1}^N(y_\alpha -
  ax_\alpha-b)(-1) = a \sum_{\alpha=1}^N x_\alpha +
  b\sum_{\alpha=1}^N 1 - \sum_{\alpha=1}^N y_\alpha = 0
\end{equation}

これから次の連立1次方程式を得る．
\begin{equation}
 \begin{pmatrix}
  \sum_{\alpha=1}^N x_\alpha ^2 & \sum_{\alpha=1}^N x_\alpha \\
  \sum_{\alpha=1}^N x_\alpha & \sum_{\alpha=1}^N 1
 \end{pmatrix}
 \begin{pmatrix}
  a \\ b
 \end{pmatrix}
=
  \begin{pmatrix}
   \sum_{\alpha=1}^N x_\alpha y_\alpha \\ \sum_{\alpha=1}^N y_\alpha
  \end{pmatrix}
\end{equation}

これを\textgt{正規方程式}と呼ぶ．これを解いて$a, b$が定まる．

\subsection{2次の最小二乗法の正規方程式}
\begin{center}
\begin{tabular}{|c|} \hline
 $N$個のデータ$(x_1, y_1), \ldots, (x_N, y_N)$に2次式$y=ax^2+bx+c$を当てはめよ．
 \\ \hline
\end{tabular}
\end{center}

\vspace{5mm}
(解)
当てはめる2次式を$y=ax^2+bx+c$とし，
\begin{equation}
 y_\alpha \approx ax_\alpha^2+bx+c, \hspace{0.7cm} \alpha=1,\ldots, N 
\end{equation}
となる$a, b, c$を最小二乗法
\begin{equation}
 J(a,b,c)=\frac{1}{2}\sum_{\alpha=1}^N(y_\alpha - (ax_\alpha^2+bx+c))^2 \rightarrow
  \min
  \label{equ:min_2ji}
\end{equation}
によって定める．それには
\begin{equation}
 \frac{\partial J(a,b,c)}{\partial a}=0, \hspace{0.7cm} \frac{\partial
  J(a,b,c)}{\partial b}=0, \hspace{0.7cm} \frac{\partial
  J(a,b,c)}{\partial c}=0
\end{equation}
を解いて$a, b, c$を定めればよい．式(\ref{equ:min_2ji})を$a, b, c$でそれ
ぞれ偏微分すると次式を得る．

\[
  \frac{\partial J(a,b,c)}{\partial a} = \sum_{\alpha=1}^N(y_\alpha -
  ax_\alpha^2-bx-c)(-x_\alpha^2) = a \sum_{\alpha=1}^N x_\alpha ^4 +
  b\sum_{\alpha=1}^N x_\alpha^3 + c\sum_{\alpha=1}^N x_\alpha^2
  - \sum_{\alpha=1}^N x_\alpha^2 y_\alpha = 0
\]
\[
  \frac{\partial J(a,b,c)}{\partial b} = \sum_{\alpha=1}^N(y_\alpha -
  ax_\alpha^2-bx_\alpha-c)(-x\alpha) = a \sum_{\alpha=1}^N x_\alpha^3 +
  b\sum_{\alpha=1}^N x_\alpha^2 + c\sum_{\alpha=1}^N x_\alpha
  - \sum_{\alpha=1}^N x_\alpha y_\alpha = 0
\]
\begin{equation}
  \frac{\partial J(a,b,c)}{\partial c} = \sum_{\alpha=1}^N(y_\alpha -
  ax_\alpha^2-bx_\alpha-c)(-1) = a \sum_{\alpha=1}^N x_\alpha^2 +
  b\sum_{\alpha=1}^N x_\alpha + c\sum_{\alpha=1}^N 1
  - \sum_{\alpha=1}^N y_\alpha = 0
\end{equation}

これから次の連立1次方程式を得る．

\begin{equation}
 \begin{pmatrix}
  \sum_{\alpha=1}^N x_\alpha ^4 & \sum_{\alpha=1}^N x_\alpha^3 &
  \sum_{\alpha=1}^N x_\alpha^2 \\

  \sum_{\alpha=1}^N x_\alpha ^3 & \sum_{\alpha=1}^N x_\alpha^2 &
  \sum_{\alpha=1}^N x_\alpha \\

  \sum_{\alpha=1}^N x_\alpha ^2 & \sum_{\alpha=1}^N x_\alpha &
  \sum_{\alpha=1}^N 1\\
 \end{pmatrix}
 \begin{pmatrix}
  a \\ b \\ c
 \end{pmatrix}
=
  \begin{pmatrix}
   \sum_{\alpha=1}^N x_\alpha^2 y_\alpha \\ \sum_{\alpha=1}^N x_\alpha
   y_\alpha \\ \sum_{\alpha=1}^N y_\alpha
  \end{pmatrix}
\end{equation}
これを解いて，$a, b, c$が定まる．

\subsection{$n$次の最小二乗法の正規方程式}
\begin{center}
\begin{tabular}{|c|} \hline
 $N$個のデータ$(x_1, y_1), \ldots, (x_N, y_N)$に$n$次式\\
 $y=c_0x^n+c_1x^{n-1}+ \ldots +c_n$を当てはめよ．
 \\ \hline
\end{tabular}
\end{center}

\vspace{5mm}
(解)
当てはめる$n$次式を$y=c_0x^n+c_1x^{n-1}+ \ldots +c_n$とし，
\begin{equation}
 y_\alpha \approx c_0x_\alpha^n+c_1x_\alpha^{n-1}+ \ldots +c_n, \hspace{0.7cm} \alpha=1,\ldots, N 
\end{equation}
となる$c_1, \ldots, c_n$を最小二乗法
\begin{equation}
 J(c_0,\cdots,c_n)=\frac{1}{2}\sum_{\alpha=1}^N(y_\alpha - (c_0x_\alpha^n+c_1x_\alpha^{n-1}+ \ldots +c_n))^2 \rightarrow
  \min
  \label{equ:min_nji}
\end{equation}
によって定める．それには
\begin{equation}
 \frac{\partial J(c_0,\cdots,c_n)}{\partial c_0}=0, \hspace{0.7cm} \frac{\partial
  J(c_0,\cdots,c_n)}{\partial c_1}=0,  \hspace{0.7cm} \ldots, \hspace{0.7cm} \frac{\partial
  J(c_0,\cdots,c_n)}{\partial c_n}=0
\end{equation}
を解いて$c_1, \ldots, c_n$を定めればよい．式(\ref{equ:min_nji})を$c_k$で
偏微分すると次式を得る．

\[
  \frac{\partial J}{\partial c_k} = \sum_{\alpha=1}^N(y_\alpha
  -c_0 x_\alpha^n -c_1 x_\alpha^{n-1} - \cdots - c_n)(-x_\alpha^{n-k}) 
\]
\begin{equation}
= c_0 \sum_{\alpha=1}^N x_\alpha^{2n-k}
+ c_1 \sum_{\alpha=1}^N x_\alpha^{2n-k-1}
\cdots
+ c_n \sum_{\alpha=1}^N x_\alpha^{n-k}
- \sum_{\alpha=1}^N x_\alpha^{n-k}y_\alpha
\end{equation}


これを$0$と置いて$k=0, 1, \ldots, n$に対する式を並べると次の正規方程式を
得る．

\begin{equation}
 \begin{pmatrix}
  \sum_{\alpha=1}^N x_\alpha^{2n} & 
  \sum_{\alpha=1}^N x_\alpha^{2n-1} &
  \cdots &
  \sum_{\alpha=1}^N x_\alpha^{n} \\

  \sum_{\alpha=1}^N x_\alpha^{2n-1} & 
  \sum_{\alpha=1}^N x_\alpha^{2n-2} &
  \cdots &
  \sum_{\alpha=1}^N x_\alpha^{n-1} \\

  \vdots &
  \vdots &
  \ddots &
  \vdots \\

  \sum_{\alpha=1}^N x_\alpha^{n} & 
  \sum_{\alpha=1}^N x_\alpha^{n-1} &
  \cdots &
  \sum_{\alpha=1}^N 1 \\

 \end{pmatrix}
 \begin{pmatrix}
  c_0 \\ c_1 \\ \vdots \\ c_n
 \end{pmatrix}
=
  \begin{pmatrix}
   \sum_{\alpha=1}^N x_\alpha^{n} y_\alpha \\
   \sum_{\alpha=1}^N x_\alpha^{n-1} y_\alpha \\
   \vdots \\
   \sum_{\alpha=1}^N y_\alpha
  \end{pmatrix}
\end{equation}
これを解いて，$c_1, \ldots, c_n$が定まる．

\end{document}
